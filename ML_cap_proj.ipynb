{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Problem:\n",
    "----\n",
    "SmartTech Co. needs to accurately predict laptop prices amidst a diverse market landscape to stay competitive and strategically position its products.\n",
    "\n",
    "Business Objective: \n",
    "----\n",
    "Developing a robust machine learning model that accurately predicts laptop prices based on various features, enabling SmartTech Co. to maintain competitiveness, understand market positioning, and assess brand influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-observation:\n",
    "\n",
    "\n",
    "1,some of the rows in dataset have empty, there is no data in those column, so by applying filters i have deleted those entire rows \n",
    "\n",
    "2,there is one column called \"Unnamed: 0\" so in that there is row numbers starts from '0', its not need in this project so i deleted that column \n",
    "\n",
    "\n",
    "3, and there is an unknown column that is also representing row numbers starts from '0' so i deleted that entire column\n",
    "\n",
    "4,in dataset some of rows have \"?\" in rows on these 3 columns (inche,memory,weight), so i deleted those 3 rows  \n",
    "\n",
    "the above all the steps i have done in excel the final data set is shown below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # importing pandas library to read dataset and for data manioulation\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt # importing pandas library to visualize data\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset\n",
    "df = pd.read_csv('final dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am creating multiple copies of data to not disturb the original dataset and use them as and when required.\n",
    "df1=df.copy()\n",
    "df2=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration and Understanding:\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the no. of rows and columns\n",
    "num_rows, num_columns = df.shape\n",
    "print(\"shape of dataset\",df.shape)\n",
    "print(\"size of dataset\",df.size)\n",
    "print(\"dimensions of dataset\",df.ndim)\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\") # to check the statistics of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # to check the number of missing values in each of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplitates remove \n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Number of rows after removing duplicates:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need of finding outliers in this context because:\n",
    "\n",
    "In the context of laptops, it's common for there to be a wide range of screen sizes ('Inches'), RAM sizes ('Ram'), and weights ('Weight') across different laptop models and manufacturers. These variations are typically considered normal and may not necessarily indicate outliers\n",
    "\n",
    "here the outliers represent unique high-end or niche laptops, keeping them might help the model capture the price variations associated with those specific features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA(Exploratory Data Analysis)\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ram'] = df['Ram'].str.replace('GB','')\n",
    "df['Weight'] = df['Weight'].str.replace('kg','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ram'] = df['Ram'].astype('int32')\n",
    "df['Weight'] = df['Weight'].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns=df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "print(\"Columns with object data type:\", object_columns)\n",
    "print(\"Columns with int and float data type:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " univerient analsyis\n",
    " 1. Distribution of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Price'], kde=True)\n",
    "plt.title('Histogram of Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most people buying laptop in between 20000 to 50000\n",
    "\n",
    "the cost incresing the customers also decreasing\n",
    "\n",
    "and Price column have left-skewed we have to normalize it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "here the target column is left-skewed or negatively skewed \n",
    "\n",
    "so it can lead to biased model predictions\n",
    "\n",
    "so for that reason i am applying logarithmic transformation (log) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(np.log(df['Price']), kde=True)\n",
    "plt.title('Histogram of Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Inches'], kde=True)\n",
    "plt.title('Histogram of Inches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Ram'], kde=True)\n",
    "plt.title('Histogram of Ram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['Weight'], kde=True)\n",
    "plt.title('Histogram of Weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Distribution of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Company'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the people buying Lenovo and Dell companies laptops \n",
    "\n",
    "may be the reason is those are on budgets we will confirm this by doing \"Bivariate analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TypeName'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the people buying Notebook Type laptops \n",
    "\n",
    "because these are designed for general-purpose use, including productivity tasks, multimedia consumption, and light gaming\n",
    "\n",
    "so most of the people who are buying laptop for general purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OpSys'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Bivariate analysis\n",
    " 1. NUM - NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "    if i=='Price':\n",
    "        pass\n",
    "    else:\n",
    "        correlation = df[i].corr(df['Price'])\n",
    "        print(f\"Correlation between {i} and Price\", correlation)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only Ram and price have more +ve Correlation compared to other numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=df['Ram'],y=df['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CAT-NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cat\",object_columns)\n",
    "print(\"Num\",numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df['Company'],y=df['Price'],palette=\"colorblind\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see most people buying lenovo and dell laptops here we can see their price are below 60000 only \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df['TypeName'],y=df['Price'],palette=\"bright\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering:\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have multiple data in ScreenResolution like Full HD or not,Touchscreen or not,IPS Panel or not etc..\n",
    "df['ScreenResolution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st crating a new column and representing 1 if 'Touchscreen' is present, if not 0\n",
    "\n",
    "touchscreen_list = []\n",
    "\n",
    "# Iterate over each value in the 'ScreenResolution' column\n",
    "for resolution in df['ScreenResolution']:\n",
    "    # Check if 'Touchscreen' is present in the resolution string\n",
    "    if 'Touchscreen' in resolution:\n",
    "        # Append 1 to the list if 'Touchscreen' is present\n",
    "        touchscreen_list.append(1)\n",
    "    else:\n",
    "        # Append 0 to the list if 'Touchscreen' is not present\n",
    "        touchscreen_list.append(0)\n",
    "\n",
    "# Creating a new column 'Touchscreen' in the DataFrame and assign the list to it\n",
    "\n",
    "df['Touchscreen'] = touchscreen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see is there any relation in between Touchscreen and Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Touchscreen',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the laptops didnt have touchscreen\n",
    "\n",
    "because of highprices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df['Touchscreen'],y=df['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the average price of touchscreen laptops are in between 70000 to 80000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next crating a new column and representing 1 if 'IPS' is present, if not 0\n",
    "\n",
    "#Ips (In-Plane Switching) - IPS panels are known for their superior color accuracy, \n",
    "# wide viewing angles, and excellent image quality compared to other panel technologies\n",
    "\n",
    "IPS_list = []\n",
    "\n",
    "for resolution in df['ScreenResolution']:\n",
    "    \n",
    "    if 'IPS' in resolution:\n",
    "        IPS_list.append(1)\n",
    "    else:\n",
    "        IPS_list.append(0)\n",
    "\n",
    "\n",
    "df['Ips'] = IPS_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Ips',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=df['Ips'],y=df['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in ScreenResolution we have horizontal and vertical resolution of the display in pixels by using this we can calculate Pixels Per Inch (PPI)\n",
    "\n",
    "Laptops with higher PPI displays may be positioned as premium products compared to those with lower PPI displays\n",
    "\n",
    "PPI formula : you need to know the horizontal and vertical resolution of the display in pixels, as well as the diagonal size of the display in inches.\n",
    "\n",
    "PPI= ((horizontal resolution)**2 + (vertical resolution)**2)**0.5/diagonal size(inches)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = df['ScreenResolution'].str.split('x',n=1,expand=True)\n",
    "df['X_res'] = new[0].str[-4:].astype('int')\n",
    "df['Y_res'] = new[1].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ppi'] = (((df['X_res']**2) + (df['Y_res']**2))**0.5/df['Inches']).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns\n",
    "numerical_columns = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calculate correlation\n",
    "numeric_columns_correlation = numerical_columns.corr()['Price']\n",
    "\n",
    "print(numeric_columns_correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->now all the data from \"ScreenResolution\" we taken into different columns, and we can drop this column \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ScreenResolution'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next column is \"Cpu\" in this also we have lot of data \n",
    "\n",
    "we have some of categories in \"Cpu\" lets divide them 1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cpu'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_names = []  # List to store processed CPU names\n",
    "\n",
    "\n",
    "for cpu in df['Cpu']:\n",
    "    x =cpu.split()[0:3]\n",
    "    cpu_names.append(\" \".join(x))\n",
    "\n",
    "# Creating a new column 'Cpu Name' in the DataFrame and assign the processed CPU names\n",
    "df['Cpu Name'] = cpu_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from \"Cpu name\" column we can fetch easily the processor\n",
    "\n",
    "so here we have mainly 5 processors\n",
    "\n",
    "Intel Core i3,\n",
    "Intel Core i5,\n",
    "Intel Core i7,\n",
    "AMD Processor,\n",
    "Other Intel Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_processor(text):\n",
    "    if text == 'Intel Core i7' or text == 'Intel Core i5' or text == 'Intel Core i3':\n",
    "        return text\n",
    "    else:\n",
    "        if text.split()[0] == 'Intel':\n",
    "            return 'Other Intel Processor'\n",
    "        else:\n",
    "            return 'AMD Processor'\n",
    "\n",
    "processor = []  # List to store processed CPU brands\n",
    "\n",
    "# Iterate over each value in the 'Cpu Name' column\n",
    "for cpu_name in df['Cpu Name']:\n",
    "    processor.append(fetch_processor(cpu_name))\n",
    "\n",
    "# Create a new column 'Cpu brand' in the DataFrame and assign the processed CPU brands\n",
    "df['processor'] = processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processor'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['processor'],y=df['Price'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so based on Processor price is varying \n",
    "\n",
    "here we have taken all the data from \"Cpu\" column we can drop these columns \n",
    "\n",
    "\"Cpu\"\n",
    "\"Cpu name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Cpu','Cpu Name'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next column is \"Ram\" , we have strong correlation between ram and price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df['Ram'].corr(df['Price'])\n",
    "print(\"Correlation between Ram and Price:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ram'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Ram'],y=df['Price'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next column memory we have more information \n",
    "\n",
    "differnt types of there \n",
    "\n",
    "1,SSD\n",
    "\n",
    "2,HDD\n",
    "\n",
    "3,Flash Storage\n",
    "\n",
    "4,combination of SSD and HDD\n",
    "\n",
    "5,hybride\n",
    "\n",
    "etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Memory'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Preprocessing\n",
    "\n",
    "df['Memory'] = df['Memory'].astype(str).replace('\\.0', '', regex=True)\n",
    "df[\"Memory\"] = df[\"Memory\"].str.replace('GB', '')\n",
    "df[\"Memory\"] = df[\"Memory\"].str.replace('TB', '000')\n",
    "new = df[\"Memory\"].str.split(\"+\", n=1, expand=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"first\"]= new[0]\n",
    "df[\"first\"]=df[\"first\"].str.strip()\n",
    "df[\"second\"]= new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Layer1HDD\"] = df[\"first\"].apply(lambda x: 1 if \"HDD\" in x else 0)\n",
    "df[\"Layer1SSD\"] = df[\"first\"].apply(lambda x: 1 if \"SSD\" in x else 0)\n",
    "df[\"Layer1Hybrid\"] = df[\"first\"].apply(lambda x: 1 if \"Hybrid\" in x else 0)\n",
    "df[\"Layer1Flash_Storage\"] = df[\"first\"].apply(lambda x: 1 if \"Flash Storage\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"second\"].fillna(\"0\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Layer2HDD\"] = df[\"second\"].apply(lambda x: 1 if \"HDD\" in x else 0)\n",
    "df[\"Layer2SSD\"] = df[\"second\"].apply(lambda x: 1 if \"SSD\" in x else 0)\n",
    "df[\"Layer2Hybrid\"] = df[\"second\"].apply(lambda x: 1 if \"Hybrid\" in x else 0)\n",
    "df[\"Layer2Flash_Storage\"] = df[\"second\"].apply(lambda x: 1 if \"Flash Storage\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first'] = df['first'].str.replace('\\D', '',regex=True)\n",
    "df['second'] = df['second'].str.replace('\\D', '',regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"first\"] = df[\"first\"].astype(int)\n",
    "df[\"second\"] = df[\"second\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Memory',\"first\",\"second\",\"Layer1HDD\",\"Layer1SSD\",\"Layer1Hybrid\",\"Layer1Flash_Storage\",\"Layer2HDD\",\"Layer2SSD\",\"Layer2Hybrid\",\"Layer2Flash_Storage\"]\n",
    "subset_df = df.loc[:, selected_columns]\n",
    "print(subset_df.sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"HDD\"]=(df[\"first\"]*df[\"Layer1HDD\"]+df[\"second\"]*df[\"Layer2HDD\"])\n",
    "df[\"SSD\"]=(df[\"first\"]*df[\"Layer1SSD\"]+df[\"second\"]*df[\"Layer2SSD\"])\n",
    "df[\"Hybrid\"]=(df[\"first\"]*df[\"Layer1Hybrid\"]+df[\"second\"]*df[\"Layer2Hybrid\"])\n",
    "df[\"Flash_Storage\"]=(df[\"first\"]*df[\"Layer1Flash_Storage\"]+df[\"second\"]*df[\"Layer2Flash_Storage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Memory',\"HDD\",\"SSD\",\"Hybrid\",\"Flash_Storage\"]\n",
    "subset_df = df.loc[:, selected_columns]\n",
    "print(subset_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above we have taken memory column and arranged HDD, SSD ,Hybrid ,Flash_Storage\n",
    "\n",
    "we can drop these columns now \n",
    "\n",
    "'first',\n",
    "'second', \n",
    "'Layer1HDD', \n",
    "'Layer1SSD', \n",
    "'Layer1Hybrid',\n",
    "'Layer1Flash_Storage', \n",
    "'Layer2HDD',\n",
    "'Layer2SSD',\n",
    "'Layer2Hybrid',\n",
    "'Layer2Flash_Storage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Memory','first', 'second', 'Layer1HDD', 'Layer1SSD', 'Layer1Hybrid',\n",
    "       'Layer1Flash_Storage', 'Layer2HDD', 'Layer2SSD', 'Layer2Hybrid',\n",
    "       'Layer2Flash_Storage'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calculate correlation\n",
    "numeric_columns_correlation = numerical_columns.corr()['Price']\n",
    "\n",
    "print(numeric_columns_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you see correlation between price and the below columns \n",
    "\n",
    "HDD    -0.095735\n",
    "\n",
    "SSD              0.668174\n",
    "\n",
    "Hybrid           0.022703\n",
    "\n",
    "Flash_Storage   -0.034721\n",
    "\n",
    "Hybrid and Flash_Storage have weak relation and also most of the laptops dont have this memory so we can drop these 2 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Hybrid','Flash_Storage'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next column is \"GPU\" , in this also we have different catogires, for simplefication what i do is like \n",
    "\n",
    "i am going to create one column called Gpu brand in that \n",
    "\n",
    "all the intel related Gpu as intel brand\n",
    "\n",
    "all the AMD related Gpu as AMD brand\n",
    "\n",
    "all the Nvidia related Gpu as Nvidia brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gpu'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gpu_brand'] = df['Gpu'].apply(lambda x:x.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need of Gpu column \n",
    "df.drop(columns=['Gpu'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gpu_brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Gpu_brand'] != 'ARM']\n",
    "df['Gpu_brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['Gpu_brand'],y=df['Price'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see there is relation between Gpu brand and price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next column is \"Opsys\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OpSys'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have so many categories in this column lets create one new column called \"OS\"\n",
    "\n",
    "in \"OS\" i divided like below\n",
    "\n",
    "Windows 10, Windows10 S, Windows 7 as Windows \n",
    "\n",
    "macOS, Mac OS as Mac\n",
    "\n",
    "other os as Others/No OS/Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_os(text):\n",
    "    if text == 'Windows 10' or text == 'Windows 7' or text == 'Windows 10 S':\n",
    "        return 'Windows'\n",
    "    elif text == 'macOS' or text == 'Mac OS X':\n",
    "        return 'Mac'\n",
    "    else:\n",
    "        return 'Others/No OS/Linux'\n",
    "\n",
    "os = []  # List to store processed os names\n",
    "\n",
    "# Iterate over each value in the 'OpSys' column\n",
    "for os_name in df['OpSys']:\n",
    "    os.append(cat_os(os_name))\n",
    "\n",
    "# Create a new column 'os' in the DataFrame and assign the processed os names\n",
    "df['os'] = os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can drop Opsys column\n",
    "\n",
    "df.drop(columns=['OpSys'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['os'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=df['os'],y=df['Price'])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we can see the windows and other os laptop price are less compare to Mac\n",
    "\n",
    "so most of people buying windows and other os \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calculate correlation\n",
    "numeric_columns_correlation = numerical_columns.corr()['Price']\n",
    "\n",
    "print(numeric_columns_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have somany features but we dont need these many feature \n",
    "\n",
    "so i am droping the 'Inches','X_res', 'Y_res' because we already find the PPI \n",
    "\n",
    "PPI and Price correlation also strong \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Inches','X_res', 'Y_res'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis Testing\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis Testing\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gpu_brand'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA Test:(Gpu_brand vs Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Extract numerical variable for ANOVA test\n",
    "group1_data = df[df['Gpu_brand'] == 'Intel']['Price']\n",
    "group2_data = df[df['Gpu_brand'] == 'AMD']['Price']\n",
    "group3_data = df[df['Gpu_brand'] == 'Nvidia']['Price']\n",
    "\n",
    "# Perform ANOVA test\n",
    "f_statistic, p_value = f_oneway(group1_data, group2_data, group3_data)\n",
    "\n",
    "# Print the results\n",
    "print(\"ANOVA Test:\")\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis - There are significant differences between the means of the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - There are no significant differences between the means of the groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANOVA Test:(OS vs Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['os'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Extract numerical variable for ANOVA test\n",
    "group1_data = df[df['os'] == 'Mac']['Price']\n",
    "group2_data = df[df['os'] == 'Others/No OS/Linux']['Price']\n",
    "group3_data = df[df['os'] == 'Windows']['Price']\n",
    "\n",
    "# Perform ANOVA test\n",
    "f_statistic, p_value = f_oneway(group1_data, group2_data, group3_data)\n",
    "\n",
    "# Print the results\n",
    "print(\"ANOVA Test:\")\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis - There are significant differences between the means of the groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis - There are no significant differences between the means of the groups.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calculate correlation\n",
    "numeric_columns_correlation = numerical_columns.corr()['Price']\n",
    "\n",
    "print(numeric_columns_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = numerical_columns.corr()\n",
    "sns.heatmap(correlation_matrix,cmap='coolwarm',annot=True, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final DataSet\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Development:\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error ,r2_score\n",
    "\n",
    "x = df.drop(columns=['Price'])\n",
    "y = np.log(df['Price'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features=['Ram', 'Weight','Touchscreen', 'Ips', 'ppi', 'HDD', 'SSD']\n",
    "categorical_features=['Company', 'TypeName', 'processor', 'Gpu_brand', 'os']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Model Training\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'GradientBoostingRegressor':GradientBoostingRegressor (random_state=42)\n",
    "}\n",
    "mse_dict = {}\n",
    "mae_dict ={}\n",
    "r2_dict={}\n",
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    model_pipeline.fit(x_train, y_train)\n",
    "    y_pred = model_pipeline.predict(x_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    R2score = r2_score(y_test,y_pred)\n",
    "    \n",
    "    \n",
    "    mse_dict[name] = mse\n",
    "    mae_dict[name] = mae\n",
    "    r2_dict[name] = R2score\n",
    "    print(f\"{name} MSE: {mse}\")\n",
    "    print(f\"{name} MAE: {mae}\")\n",
    "    print(f\"{name} R2score: {R2score}\")\n",
    "    print(\"----------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here i have choosen Random Forest algorithms to predict laptop prices.\n",
    "\n",
    "Random Forest MSE: 0.04932216134762586\n",
    "\n",
    "Random Forest MAE: 0.16173778229225444\n",
    "\n",
    "Random Forest R2score: 0.8585956995981013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    model_pipeline.fit(x_train, y_train)\n",
    "    y_pred = model_pipeline.predict(x_test)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # sns.scatterplot(x=y_test, y=y_pred, label='Predicted price')\n",
    "    sns.regplot(x = y_test, y=y_pred, label='Predicted price')\n",
    "    plt.plot(np.arange(y_test.min(), y_test.max()), np.arange(y_test.min(), y_test.max()), color='red', label='Perfect Prediction')\n",
    "    plt.xlabel('Actual df')\n",
    "    plt.ylabel('Predicted df')\n",
    "    plt.title(f'{name}:Predicted vs. Actual df')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_columns=df.select_dtypes(exclude=['object']).columns.tolist()\n",
    "print(\"Columns with object data type:\", object_columns)\n",
    "print(\"Columns with int and float data type:\", numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=['Price'])\n",
    "y = np.log(df['Price'])\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "numeric_features = ['Ram', 'Weight', 'HDD', 'SSD']\n",
    "categorical_features = ['Company', 'TypeName', 'processor', 'Gpu_brand', 'os']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "def predict_price(Company, TypeName, Ram, Weight, Inches, X_res, Y_res, Touchscreen, Ips, processor, HDD, SSD, Gpu_brand, os):\n",
    "    \n",
    "    ppi = (X_res**2 + Y_res**2)**0.5 / Inches\n",
    "     \n",
    "    input_data = pd.DataFrame({\n",
    "        'Company': [Company],\n",
    "        'TypeName': [TypeName],\n",
    "        'Ram': [Ram],\n",
    "        'Weight': [Weight],\n",
    "        'Touchscreen': [Touchscreen],\n",
    "        'Ips': [Ips],\n",
    "        'processor': [processor],\n",
    "        'HDD': [HDD],\n",
    "        'SSD': [SSD],\n",
    "        'Gpu_brand': [Gpu_brand],\n",
    "        'os': [os],\n",
    "        'ppi': [ppi]  \n",
    "    })\n",
    "\n",
    "    \n",
    "    predicted_log_price = pipeline.predict(input_data)[0]\n",
    "    predicted_price = np.exp(predicted_log_price)\n",
    "    return round(predicted_price)\n",
    "\n",
    "company_list = df[\"Company\"].unique().tolist()\n",
    "type_name_list = df[\"TypeName\"].unique().tolist()\n",
    "Ram_list = sorted(df[\"Ram\"].unique().tolist()) \n",
    "processor_list = df[\"processor\"].unique().tolist()\n",
    "HDD_list = sorted(df[\"HDD\"].unique().tolist())\n",
    "SSD_list = sorted(df[\"SSD\"].unique().tolist()) \n",
    "gpu_brand_list = df[\"Gpu_brand\"].unique().tolist()\n",
    "os_list = df[\"os\"].unique().tolist()\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=predict_price,\n",
    "    inputs=[\n",
    "        gr.Dropdown(choices=company_list, label=\"Company\"),\n",
    "        gr.Radio(choices=type_name_list, label=\"Type Name\"),\n",
    "        gr.Dropdown(choices=Ram_list, label=\"RAM (GB)\"),\n",
    "        gr.Number(label=\"Weight (kg)\"),\n",
    "        gr.Number(label=\"Screen Size (inches)\"),\n",
    "        gr.Number(label=\"X resolution\"),\n",
    "        gr.Number(label=\"Y resolution\"),\n",
    "        gr.Checkbox(label=\"Touchscreen\"),\n",
    "        gr.Checkbox(label=\"IPS Display\"),\n",
    "        gr.Radio(choices=processor_list, label=\"Processor\"),\n",
    "        gr.Dropdown(choices=HDD_list, label=\"HDD Storage (GB)\"),\n",
    "        gr.Dropdown(choices=SSD_list, label=\"SSD Storage (GB)\"),\n",
    "        gr.Radio(choices=gpu_brand_list, label=\"GPU Brand (if applicable)\"),\n",
    "        gr.Radio(choices=os_list, label=\"Operating System\"),\n",
    "    ],\n",
    "    outputs=[gr.Textbox(label=\"Estimated Price\")],\n",
    "    title=\"Laptop Price Prediction App\",\n",
    "    description=\"Get an estimated price for your desired laptop based on its specifications.\",\n",
    "    allow_flagging=False  # Allow for feedback and model improvement\n",
    ")\n",
    "\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to Explore:\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1,Which features have the most significant impact on laptop prices?\n",
    "\n",
    "below are the most significant impact on laptop prices\n",
    "\n",
    "1,Ram (0.683342): This suggests that as the RAM (memory) capacity increases, the price of the laptop tends to increase as well.\n",
    "\n",
    "2,SSD (0.668020): This indicates that laptops with higher SSD storage capacities tend to have higher prices.\n",
    "\n",
    "3,ppi (0.467391): Laptops with higher pixel densities may be associated with higher prices, possibly due to better display quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = df.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calculate correlation\n",
    "numeric_columns_correlation = numerical_columns.corr()['Price']\n",
    "\n",
    "print(numeric_columns_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2,Can the model accurately predict the prices of laptops from lesser-known brands?\n",
    "\n",
    "->here in this dataset we dont have more data about lesser-known brands so thats why i didnt replace outliers because if i replace those machine will be strict to prdict only most-know or which brands have more data in dataset \n",
    "\n",
    "->now my modal will predict most near to Prices of all types of brands \n",
    "\n",
    "->if we have more data we can Divide our dataset into subsets based on these categories. \n",
    "\n",
    "->then if we can train on that data we will get most accurate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3,Does the brand of the laptop significantly influence its price?\n",
    "\n",
    "yes,brand of the laptop significantly influence its price\n",
    "\n",
    "brands like HP,asus,dell,lenovo are midrange brand value \n",
    "\n",
    "but if you see Apple,MSI,microsoft have above midrange brand value \n",
    "\n",
    "small brands have price less only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 408, in run_asgi\n",
      "  |     result = await app(  # type: ignore[func-returns-value]\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 69, in __call__\n",
      "  |     return await self.app(scope, receive, send)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fastapi\\applications.py\", line 1054, in __call__\n",
      "  |     await super().__call__(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\applications.py\", line 123, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "  |     raise exc\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "  |     await self.app(scope, receive, _send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gradio\\route_utils.py\", line 689, in __call__\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 62, in __call__\n",
      "  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 758, in __call__\n",
      "  |     await self.middleware_stack(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 778, in app\n",
      "  |     await route.handle(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 299, in handle\n",
      "  |     await self.app(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 79, in app\n",
      "  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 64, in wrapped_app\n",
      "  |     raise exc\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "  |     await app(scope, receive, sender)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\routing.py\", line 77, in app\n",
      "  |     await response(scope, receive, send)\n",
      "  |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\responses.py\", line 257, in __call__\n",
      "  |     async with anyio.create_task_group() as task_group:\n",
      "  |   File \"C:\\Users\\shaik\\AppData\\Roaming\\Python\\Python312\\site-packages\\anyio\\_backends\\_asyncio.py\", line 664, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\responses.py\", line 260, in wrap\n",
      "    |     await func()\n",
      "    |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\responses.py\", line 254, in stream_response\n",
      "    |     await send({\"type\": \"http.response.body\", \"body\": b\"\", \"more_body\": False})\n",
      "    |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    |     await send(message)\n",
      "    |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\_exception_handler.py\", line 50, in sender\n",
      "    |     await send(message)\n",
      "    |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    |     await send(message)\n",
      "    |   File \"c:\\Users\\shaik\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 461, in send\n",
      "    |     raise ClientDisconnected\n",
      "    | uvicorn.protocols.utils.ClientDisconnected\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sns.barplot(x=df['Company'],y=df['Price'],palette=\"colorblind\")\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4,How well does the model perform on laptops with high-end specifications compared to budget laptops?\n",
    "\n",
    "->here in this dataset we dont have more data about laptops with high-end specifications,so thats why i didnt replace outliers because if i replace those, machine will be strict to prdict only budget laptops \n",
    "\n",
    "->now my modal will predict most near to Prices of all types of brands \n",
    "\n",
    "->if we have more data about laptops with high-end specifications then we can divided dataset into subsets based on these categories. \n",
    "\n",
    "->then if we can train on that data then our model will perform on laptops with high-end specifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5,What are the limitations and challenges in predicting laptop prices accurately?\n",
    "\n",
    "limitations:\n",
    "\n",
    "->Limited availability and quality of data ,if we have more data we can train model well\n",
    "\n",
    "->these market have everyday new things into picture like new technology,high-GPU,Display updatation so its difficult to train model \n",
    "we have to take care of all these things \n",
    "\n",
    "challenges:\n",
    "\n",
    "->Laptop prices are influenced by a wide range of factors, including specifications (e.g., processor type, RAM size, storage capacity), brand reputation, market demand, technological advancements, and economic factors. Capturing all these factors accurately in a predictive model can be big challenge\n",
    "\n",
    "->Outliers the data, such as pricing errors, promotional discounts, or limited-time offers, can distort the predictive model's performance and lead to inaccurate predictions if not handled properly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6,How does the model perform when predicting the prices of newly released laptops not present in the training dataset?\n",
    "\n",
    "->if Newly released laptops have the same futures as dataset then it will perform like normal only \n",
    "\n",
    "->if Newly released laptops may have unique features or specifications that are not present in the training dataset. As a result, the model may struggle to find comparable products to base its predictions on, leading to less reliable estimates.\n",
    "\n",
    "->solution is like continuously update the model with new data as it becomes available\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
